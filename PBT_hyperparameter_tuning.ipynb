{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from security_env import SecurityEnv\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "import copy\n",
    "\n",
    "class PBTAgent:\n",
    "    def __init__(self, params: Dict[str, Any]):\n",
    "        self.params = params\n",
    "        self.model = None\n",
    "        self.reward = float('-inf')\n",
    "        self.steps = 0\n",
    "    \n",
    "    def create_env(self):\n",
    "        return SecurityEnv(\n",
    "            rf_model_path=\"fatigue_model.joblib\",\n",
    "            alpha=self.params['alpha'],\n",
    "            beta=self.params['beta'],\n",
    "            s_min=self.params['s_min']\n",
    "        )\n",
    "    \n",
    "    def train(self, n_steps: int):\n",
    "        env = self.create_env()\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = PPO(\n",
    "                \"MlpPolicy\",\n",
    "                env,\n",
    "                learning_rate=self.params['learning_rate'],\n",
    "                n_steps=self.params['n_steps'],\n",
    "                batch_size=self.params['batch_size'],\n",
    "                n_epochs=self.params['n_epochs'],\n",
    "                gamma=self.params['gamma'],\n",
    "                gae_lambda=self.params['gae_lambda'],\n",
    "                clip_range=self.params['clip_range'],\n",
    "                ent_coef=self.params['ent_coef'],\n",
    "                verbose=0\n",
    "            )\n",
    "        \n",
    "        self.model.learn(total_timesteps=n_steps)\n",
    "        self.steps += n_steps\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_env = self.create_env()\n",
    "        eval_env = DummyVecEnv([lambda: eval_env])\n",
    "        self.reward, _ = evaluate_model(self.model, eval_env)\n",
    "    \n",
    "    def exploit(self, other_agent: 'PBTAgent'):\n",
    "        \"\"\"Copy parameters from better performing agent\"\"\"\n",
    "        self.params = copy.deepcopy(other_agent.params)\n",
    "        self.model = copy.deepcopy(other_agent.model)\n",
    "    \n",
    "    def explore(self):\n",
    "        \"\"\"Randomly perturb parameters\"\"\"\n",
    "        for key in self.params:\n",
    "            if isinstance(self.params[key], float):\n",
    "                self.params[key] *= np.random.uniform(0.8, 1.2)\n",
    "            elif isinstance(self.params[key], int):\n",
    "                self.params[key] = int(self.params[key] * np.random.uniform(0.8, 1.2))\n",
    "\n",
    "def evaluate_model(model, env, n_episodes=5):\n",
    "    \"\"\"Evaluate a trained model\"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        obs = env.reset()[0]  # Get only the observation\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = env.step(action)  # Updated to handle new gymnasium API\n",
    "            episode_reward += reward\n",
    "        rewards.append(episode_reward)\n",
    "    return np.mean(rewards), np.std(rewards)\n",
    "\n",
    "def run_pbt(\n",
    "    population_size: int = 8,\n",
    "    num_generations: int = 10,\n",
    "    steps_per_generation: int = 10000,\n",
    "    exploit_threshold: float = 0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Population Based Training\n",
    "    \"\"\"\n",
    "    # Initialize population\n",
    "    population = []\n",
    "    param_ranges = {\n",
    "        'learning_rate': np.logspace(-5, -3, 100),\n",
    "        'n_steps': [2048, 4096, 8192],\n",
    "        'batch_size': [32, 64, 128],\n",
    "        'n_epochs': [5, 10, 20],\n",
    "        'gamma': [0.95, 0.99, 0.995],\n",
    "        'gae_lambda': [0.9, 0.95, 0.98],\n",
    "        'clip_range': [0.1, 0.2, 0.3],\n",
    "        'ent_coef': [0.0, 0.01, 0.005],\n",
    "        'alpha': [0.3, 0.5, 0.7],\n",
    "        'beta': [0.3, 0.5, 0.7],\n",
    "        's_min': [5.0, 8.0, 10.0]\n",
    "    }\n",
    "    \n",
    "    for _ in range(population_size):\n",
    "        params = {k: np.random.choice(v) for k, v in param_ranges.items()}\n",
    "        population.append(PBTAgent(params))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for generation in range(num_generations):\n",
    "        print(f\"\\nGeneration {generation + 1}/{num_generations}\")\n",
    "        \n",
    "        # Train all agents\n",
    "        for agent in population:\n",
    "            agent.train(steps_per_generation)\n",
    "        \n",
    "        # Sort agents by reward\n",
    "        population.sort(key=lambda x: x.reward, reverse=True)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'generation': generation,\n",
    "            'best_reward': population[0].reward,\n",
    "            'mean_reward': np.mean([a.reward for a in population]),\n",
    "            'std_reward': np.std([a.reward for a in population])\n",
    "        })\n",
    "        \n",
    "        print(f\"Best Reward: {population[0].reward:.2f}\")\n",
    "        print(f\"Mean Reward: {np.mean([a.reward for a in population]):.2f}\")\n",
    "        \n",
    "        # Exploit and explore\n",
    "        for i in range(population_size):\n",
    "            if i > 0 and np.random.random() < exploit_threshold:  # Only exploit if i > 0\n",
    "                # Exploit from better performing agent\n",
    "                better_agent = population[np.random.randint(0, i)]\n",
    "                population[i].exploit(better_agent)\n",
    "            # Explore\n",
    "            population[i].explore()\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('pbt_results.csv', index=False)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1/10\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run PBT with default parameters\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m run_pbt()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Or specify custom parameters\u001b[39;00m\n\u001b[0;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m run_pbt(\n\u001b[0;32m      6\u001b[0m     population_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m      7\u001b[0m     num_generations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      8\u001b[0m     steps_per_generation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[0;32m      9\u001b[0m     exploit_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n",
      "Cell \u001b[1;32mIn[2], line 114\u001b[0m, in \u001b[0;36mrun_pbt\u001b[1;34m(population_size, num_generations, steps_per_generation, exploit_threshold)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Train all agents\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m population:\n\u001b[1;32m--> 114\u001b[0m     agent\u001b[38;5;241m.\u001b[39mtrain(steps_per_generation)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Sort agents by reward\u001b[39;00m\n\u001b[0;32m    117\u001b[0m population\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mreward, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[2], line 49\u001b[0m, in \u001b[0;36mPBTAgent.train\u001b[1;34m(self, n_steps)\u001b[0m\n\u001b[0;32m     47\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_env()\n\u001b[0;32m     48\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: eval_env])\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward, _ \u001b[38;5;241m=\u001b[39m evaluate_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, eval_env)\n",
      "Cell \u001b[1;32mIn[2], line 73\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, env, n_episodes)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     72\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 73\u001b[0m     obs, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# Updated to handle new gymnasium API\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     75\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:207\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:59\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 59\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mstep(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m     60\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[env_idx]\n\u001b[0;32m     61\u001b[0m         )\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\Tuan Anh HSLU\\OneDrive - Hochschule Luzern\\Desktop\\HSLU22\\Bachelor Thesis\\ML Models\\security_env.py:209\u001b[0m, in \u001b[0;36mSecurityEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# Update configuration and compute scores\u001b[39;00m\n\u001b[0;32m    208\u001b[0m config \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m--> 209\u001b[0m s_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_security_score(config)\n\u001b[0;32m    210\u001b[0m afs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fatigue_score(config)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Compute reward\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tuan Anh HSLU\\OneDrive - Hochschule Luzern\\Desktop\\HSLU22\\Bachelor Thesis\\ML Models\\security_env.py:157\u001b[0m, in \u001b[0;36mSecurityEnv._compute_security_score\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, feature_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names):\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_name \u001b[38;5;129;01min\u001b[39;00m weights:\n\u001b[1;32m--> 157\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_action_to_feature_range(config[i], feature_name)\n\u001b[0;32m    158\u001b[0m         score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m*\u001b[39m weights[feature_name]\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "# Run PBT with default parameters\n",
    "results = run_pbt()\n",
    "\n",
    "# Or specify custom parameters\n",
    "results = run_pbt(\n",
    "    population_size=8,\n",
    "    num_generations=10,\n",
    "    steps_per_generation=10000,\n",
    "    exploit_threshold=0.2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
